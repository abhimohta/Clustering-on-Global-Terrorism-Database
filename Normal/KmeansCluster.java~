package abc;

import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.net.URI;
import java.util.ArrayList;
import java.util.HashSet;
import java.util.List;
import java.util.Set;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.Counter;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.StringUtils;


import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.Vector;

import org.apache.hadoop.conf.*;
import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.*;
import java.net.*;

public class KmeansCluster extends Configured implements Tool{
private static double THRESHOLD = 0.01;
private static int MAXITERATIONS = 20;
private static int index1_kmeans=0;
private static int index2_kmeans=1;
private static int index1_initK=0;
private static int index2_initK=1;
	static int boom = 1;
public static boolean stopIteration(Configuration conf) throws IOException 
	{
		FileSystem fs = FileSystem.get(conf);
		Path pervCenterFile = new Path("/initK/part-r-00000");
		Path currentCenterFile = new Path("/newCentroid/part-r-00000");
		if(!(fs.exists(pervCenterFile) && fs.exists(currentCenterFile)))
		{
			System.exit(1);
		}
		
		// delete pervCenterFile, then rename the currentCenterFile to pervCenterFile
		/* Why rename needed here: 
		 * as each iteration, the mapper will get centroid from Path("/task2/initK")
		 * but reducer will output new centroids into Path("/task2/newCentroid/part-r-00000")
		 */ 
		fs.delete(pervCenterFile,true);
		if(fs.rename(currentCenterFile, pervCenterFile) == false)
		{
			System.exit(1);
		}

		
		//check whether the centers have changed or not to determine to do iteration or not
		boolean stop=true;
		String line;
		FSDataInputStream in = fs.open(pervCenterFile);
		InputStreamReader isr = new InputStreamReader(in);
		BufferedReader br = new BufferedReader(isr);

		/* get each line, check the third parameter, if it is 0, it means this centoid is changed from last iteration, 
		 * so, we need more iteration
		 */
		while((line = br.readLine()) != null)
		{
			String []str1 = line.split(",");
			int isntChange = Integer.parseInt(str1[2].trim());
			if(isntChange < 1)
			{
				stop = false;
				break;
			}
		}
		
		return stop;
	}

    
public static class ClusterMapper extends Mapper<LongWritable, Text, Text, Text>  //output<centroid,point>
	{
		Vector<Point> centers = new Vector<Point>();
		Point point = new Point();
		int k = 0;
		private Configuration conf;
    		private BufferedReader fis;
    		    private boolean caseSensitive;
    		
	
    @Override
    public void setup(Context context) throws IOException,
        InterruptedException {
      conf = context.getConfiguration();
      caseSensitive = conf.getBoolean("wordcount.case.sensitive", true);
      if (conf.getBoolean("wordcount.skip.patterns", true)) {
        URI[] patternsURIs = Job.getInstance(conf).getCacheFiles();
        for (URI patternsURI : patternsURIs) {
          Path patternsPath = new Path(patternsURI.getPath());
          String patternsFileName = patternsPath.getName().toString();
          parseSkipFile(patternsFileName);
        }
      }
    }

    private void parseSkipFile(String fileName) {
      try {
        fis = new BufferedReader(new FileReader(fileName));
        String pattern = null;
        while ((pattern = fis.readLine()) != null) {
					point = new Point();
					String[] str = pattern.split(",");
					/*for(int i = 0;i < Point.DIMENTION; i++)
					{
						
						point.arr[i] = Integer.parseInt(str[i].trim());
					}*/
					point.arr[0]=Integer.parseInt(str[index1_initK].trim());
					point.arr[1]=Integer.parseInt(str[index2_initK].trim());
					centers.add(point);
					k++;
					if(iteration!=0){
						if(str[2].equals("0"))
							boom=1;	
					}	   	
          //patternsToSkip.add(pattern);
        }
      } catch (IOException ioe) {
        System.err.println("Caught exception while parsing the cached file '"
            + StringUtils.stringifyException(ioe));
      }
    }

  @Override
		//output<centroid,point>
		public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException 
		{
			point = new Point();
			int index = -1;
			double minDist = Double.MAX_VALUE;
			String[] str = value.toString().split(",");
			
			// load data into a point variable
			/*for(int i = 0;i < Point.DIMENTION; i++)
			{
				point.arr[i] = Integer.parseInt(str[i]);
			}*/
			
			point.arr[0]=Integer.parseInt(str[index1_kmeans]);
			point.arr[1]=Integer.parseInt(str[index2_kmeans]);
			// find the nearest centroid to this point
			for(int i = 0;i < k; i++)
			{
				double dist = Point.getEulerDist(point, centers.get(i));
				if(dist < minDist)
				{
					minDist = dist;
					index = i;
				}
			}
			double interCluster=0.0;
			double dist=0;
			for(int i=0;i<k-1;i++){
				for(int j=i+1;j<k;j++){
					dist = Point.getEulerDist(centers.get(i), centers.get(j));
					interCluster+=(dist*dist);
				}
			}
			
			double distance = Point.getEulerDist(point, centers.get(index));
			distance*=distance;
			// output the nearest centroid as key, and the piont as value
			context.write(new Text(centers.get(index).toString()), new Text(point.toString()+ ":"+String.valueOf(distance)+":"+String.valueOf(interCluster)));
		}
		
		@Override
		public void cleanup(Context context) throws IOException,InterruptedException 
		{
			// do nothing here
		}
	}
	
	// do aggregation in local side
	// add one more parameter in value field
	public static class Combiner extends Reducer<Text, Text, Text, Text> 
	{	  
		@Override
		public void reduce(Text key,Iterable<Text> values,Context context) throws IOException,InterruptedException
		{
			Point sumPoint = new Point();
			String outputValue;
			int count=0;
			double sse=0;
			double interCluster=0;
			while(values.iterator().hasNext())
			{
				String line = values.iterator().next().toString();
				String[] str = line.split(":");
				String[] pointStr = str[0].split(",");
				interCluster = Double.parseDouble(str[2]);
				/*if(str.length == 2)
				{
					count += Integer.parseInt(str[1]);
				}
				else*/
				{
					count++;
				}
				
				for(int i = 0;i < Point.DIMENTION;i ++)
				{
					sumPoint.arr[i] += Double.parseDouble(pointStr[i]);
				}
				sse+=Double.parseDouble(str[1]);
			}
			//interCluster/=count;
			outputValue = sumPoint.toString() + ":" + String.valueOf(count)+":"+String.valueOf(sse)+":"+String.valueOf(interCluster);  //value=Point_Sum+count
			context.write(key, new Text(outputValue));
		}
	}
 
	public static class UpdateCenterReducer extends Reducer<Text, Text, Text, Text> 
	{
		@Override
		public void setup(Context context)
		{
			// do nothing here
		}
 
		@Override
		public void reduce(Text key,Iterable<Text> values,Context context) throws IOException,InterruptedException
		{
			int count = 0;
			Point sumPoint = new Point();
			Point newCenterPoint = new Point();
			double sse=0;
			double inter=0;
			int count_inter=0;
			// while loop to calculate the sum of points
			while(values.iterator().hasNext())
			{
				String line = values.iterator().next().toString();
				String[] str = line.split(":");
				String[] pointStr = str[0].split(",");
				count += Integer.parseInt(str[1]);
				inter=Double.parseDouble(str[3]);
				//count_inter++;
				for(int i = 0;i < Point.DIMENTION; i++)
				{
					sumPoint.arr[i] += Double.parseDouble(pointStr[i]);
				}
				sse+=Double.parseDouble(str[2]);
			}
			
			//inter/=count_inter;

			// calculate the new centroid
			for(int i = 0; i < Point.DIMENTION; i++)
			{
				newCenterPoint.arr[i] = sumPoint.arr[i]/count;
			}
			
			// get prevois centroid
			String[] str = key.toString().split(",");
			Point preCentroid = new Point();
			for(int i = 0; i < Point.DIMENTION; i++)
			{
				preCentroid.arr[i] = Integer.parseInt(str[i]);
			}

			// compare the new & previous centroids, 
			/*
			 * If it is not "changed", make the value of the output be 1
			 * otherwise,  make the value of the output be 0
			 * the value filed will be use in stopIteration() function, which will be called in main() after each iteration
			 */
			 if(boom==1){
			if(Point.getEulerDist(preCentroid, newCenterPoint) <= THRESHOLD) 
			{
				context.write(new Text(preCentroid.toString().trim()), new Text(",1,"+String.valueOf(sse)+ "," + 															String.valueOf(count)+","+String.valueOf(inter)));
			}
			else
			{
				context.write(new Text(newCenterPoint.toString().trim()), new Text(",0,"+String.valueOf(sse)+","+ 															String.valueOf(count)+","+String.valueOf(inter)));
				boom=0;
			}
			}else{
				context.write(new Text(preCentroid.toString().trim()), new Text(",1,"+String.valueOf(sse)+ "," + 															String.valueOf(count)+","+String.valueOf(inter)));
			}
			
		}
		
		@Override
		public void cleanup(Context context) throws IOException,InterruptedException 
		{
			// do nothing here
		}
	}
	@Override
	public int run(String[] args) throws Exception 
	{
		Configuration conf = getConf();
		FileSystem fs = FileSystem.get(conf);
		Job job = Job.getInstance(conf);
		GenericOptionsParser optionParser = new GenericOptionsParser(conf, args);
    		String[] remainingArgs = optionParser.getRemainingArgs();
    		if (!(remainingArgs.length != 2 || remainingArgs.length != 4)) {
      		System.err.println("Usage: wordcount <in> <out> [-skip skipPatternFile]");
      			System.exit(2);
    		}

    		List<String> otherArgs = new ArrayList<String>();
    		for (int i=0; i < remainingArgs.length; ++i) {
      			if ("-skip".equals(remainingArgs[i])) {
       		 		job.addCacheFile(new Path(remainingArgs[++i]).toUri());
        			job.getConfiguration().setBoolean("wordcount.skip.patterns", true);
      			}else {
        			otherArgs.add(remainingArgs[i]);
      			}	
    		}
    		if(iteration==0){
    		index1_kmeans = Integer.parseInt(args[4]);
    		index2_kmeans = Integer.parseInt(args[5]);
    		index1_initK = Integer.parseInt(args[4]);
    		index2_initK = Integer.parseInt(args[5]);
    		}
    		else
    		{
		index1_kmeans = Integer.parseInt(args[4]);
    		index2_kmeans = Integer.parseInt(args[5]);
    		index1_initK = 0;
    		index2_initK = 1;
    		}
		//index1=6;
		//index2=7;
		job.setJarByClass(KmeansCluster.class);
		FileInputFormat.setInputPaths(job, args[0]);
		Path outDir = new Path("/newCentroid");
		
		FileOutputFormat.setOutputPath(job, outDir);
		fs.delete(outDir,true);
		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);
		job.setMapperClass(ClusterMapper.class);
		job.setCombinerClass(Combiner.class);
		job.setReducerClass(UpdateCenterReducer.class);
		job.setNumReduceTasks(1);// so that all new centroids will output into one file
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(Text.class);
		 
		return job.waitForCompletion(true)?0:1;
	}
	static int iteration = 0;
  	public static void main(String[] args) throws Exception {
    		Configuration conf = new Configuration();
    		boolean waitingForConverge = false;
		if(MAXITERATIONS == 0)
		{
			waitingForConverge = true;
		}
		int enableClusterOutput = 1;
		try
		{
			enableClusterOutput = Integer.parseInt(args[2]);
		}
		catch(Exception e)
		{
			enableClusterOutput = 1;
		}

		int success = 1;
		do 
		{
			success ^= ToolRunner.run(conf, new KmeansCluster(), args);
			iteration++;
			
		} while (success == 1  && !stopIteration(conf) && (waitingForConverge || iteration < MAXITERATIONS) ); 
		ToolRunner.run(conf, new KmeansCluster(), args);
		stopIteration(conf);
		// take care of the order, I make stopIteration() prior to iteration, because I must keep the initK always contain the lastest centroids after each iteration
  }
}
